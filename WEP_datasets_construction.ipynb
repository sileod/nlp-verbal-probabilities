{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5gb95Wln5_KR",
      "metadata": {
        "id": "5gb95Wln5_KR"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "try:\n",
        "    import google.colab\n",
        "    os.system('pip install problog sorcery pygrove datasets')\n",
        "except:\n",
        "    pass\n",
        "bucket=''\n",
        "usnli_path=f's3://{bucket}/usnli'\n",
        "output_path=f's3://{bucket}/probability_words'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "be24f903",
      "metadata": {
        "id": "be24f903"
      },
      "outputs": [],
      "source": [
        "import problog\n",
        "import pygrove\n",
        "from problog.program import PrologString\n",
        "from problog.core import ProbLog\n",
        "from problog import get_evaluatable\n",
        "from sorcery import dict_of\n",
        "from string import Template\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "df = pd.read_csv(\n",
        "    \"https://raw.githubusercontent.com/wadefagen/datasets/master/Perception-of-Probability-Words/survey-results.csv\"\n",
        ")\n",
        "prob_words = {\n",
        "    k.strip('\"').lower(): v / 100 for (k, v) in df.median(axis=0).to_dict().items()\n",
        "}\n",
        "prob_words[\"certain\"] = 1.0\n",
        "prob_words[\"impossible\"] = 0.0\n",
        "\n",
        "\n",
        "def get_prob_word(y):\n",
        "    p_w = sorted(list(prob_words.items()), key=lambda x: random.random())  # break ties\n",
        "    return min(p_w, key=lambda x: abs(y - x[1]))[0]\n",
        "\n",
        "\n",
        "distractors = defaultdict(list)\n",
        "for x, px in prob_words.items():\n",
        "    for y, py in prob_words.items():\n",
        "        if abs(px - py) > 0.4:\n",
        "            distractors[x] += [y]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "F_kgWZ7F9nkq",
      "metadata": {
        "id": "F_kgWZ7F9nkq"
      },
      "outputs": [],
      "source": [
        "df.median(axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "xw5grOB57f25",
      "metadata": {
        "id": "xw5grOB57f25"
      },
      "outputs": [],
      "source": [
        "# \"{expression} that {fact1}\" should be grammatical\n",
        "map={'about even': 'chances are about even',\n",
        " 'almost certain': 'it is almost certain',\n",
        " 'almost no chance': 'there is almost no chance',\n",
        " 'better than even': 'there is a better than even chance',\n",
        " 'chances are slight': 'chances are slight',\n",
        " 'highly likely': 'it is highly likely',\n",
        " 'highly unlikely': 'it is highly unlikely',\n",
        " 'improbable': 'it is improbable',\n",
        " 'likely': 'it is likely',\n",
        " 'little chance': 'there is little chance',\n",
        " 'probably': 'it is probably the case',\n",
        " 'probable': 'it is probable',\n",
        " 'probably not': 'it is probably not the case',\n",
        " 'unlikely': 'it is unlikely',\n",
        " 'very good chance': 'there is a very good chance',\n",
        " 'we believe': 'we believe',\n",
        " 'we doubt': 'we doubt',\n",
        " 'certain': 'it is certain',\n",
        "'impossible':'it is impossible'}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "tmFpRcHksWiY",
      "metadata": {
        "cellView": "form",
        "id": "tmFpRcHksWiY"
      },
      "outputs": [],
      "source": [
        "p = PrologString(\"\"\"\n",
        "and(A,B) :- A,B.\n",
        "or(A,B) :- A;B.\n",
        "nand(A,B) :- not(and(A,B)).\n",
        "nor(A,B) :- not(or(A,B)).\n",
        "xor(A,B) :- or(A,B), nand(A,B).\n",
        "\n",
        "0.3::factA.\n",
        "0.3::factB.\n",
        "0.3::factC.\n",
        "\n",
        "0.3::factX:-and(A,B).\n",
        "0.3::factY:-factA;factC.\n",
        "0.3::factZ:-factA;factC.\n",
        "\n",
        "0.3::conclusion:-or(factA,factC).\n",
        "\n",
        "query(conclusion).\n",
        "\"\"\")\n",
        "\n",
        "get_evaluatable().create_from(p).evaluate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "_FhCWShae6ae",
      "metadata": {
        "id": "_FhCWShae6ae"
      },
      "outputs": [],
      "source": [
        "def generate_template(hop=2):\n",
        "\n",
        "    second_hop = \"\"\"%hop1\n",
        "  $p4::$factX:-$op1($factX1,$factX2).\n",
        "  $p5::$factY:-$op2($factY1,$factY2).\n",
        "  $p6::$factZ:-$op3($factZ1,$factZ2).\"\"\"\n",
        "\n",
        "    if hop == 1:\n",
        "        second_hop = \"\"\n",
        "    s = f\"\"\"\n",
        "  and(A,B) :- A,B.\n",
        "  or(A,B) :- A;B.\n",
        "  nand(A,B) :- not(and(A,B)).\n",
        "  nor(A,B) :- not(or(A,B)).\n",
        "  xor(A,B) :- or(A,B), nand(A,B).\n",
        "\n",
        "  %hop0\n",
        "  $p1::$factA.\n",
        "  $p2::$factB.\n",
        "  $p3::$factC.\n",
        "  {second_hop}\n",
        "\n",
        "  %hop2\n",
        "  conclusion:-$op4($factC1,$factC2).\n",
        "\n",
        "  query(conclusion).\n",
        "  \"\"\"\n",
        "    sub = {f\"p{i}\": random.random() for i in range(1, 8)}\n",
        "    sub = {f\"p{i}\": random.choice(list(prob_words.values())) for i in range(1, 8)}\n",
        "    sub = {**sub,**{f\"op{i}\": random.choice([\"and\", \"or\", \"xor\"]) for i in range(1, 5)}}\n",
        "\n",
        "    for c in \"XYZ\":\n",
        "        sub[f\"fact{c}1\"] = random.choice([\"$factA\", \"$factB\", \"$factC\"])\n",
        "        sub[f\"fact{c}2\"] = random.choice(\n",
        "            [fact for fact in [\"$factA\", \"$factB\", \"$factC\"]\n",
        "                if fact != sub[f\"fact{c}1\"]\n",
        "            ]\n",
        "        )\n",
        "    all_facts = [\"$factA\", \"$factB\", \"$factC\", \"$factX\", \"$factY\", \"$factZ\"]\n",
        "\n",
        "    if hop == 1:\n",
        "        all_facts = [\"$factA\", \"$factB\", \"$factC\"]\n",
        "    sub[f\"factC1\"] = random.choice(all_facts)\n",
        "    sub[f\"factC2\"] = random.choice([fact for fact in all_facts if fact != sub[f\"factC1\"]])\n",
        "\n",
        "    s_ = Template(s).safe_substitute(sub).replace(\"$\", \"\")\n",
        "    return s_\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "kshFnbLgnWza",
      "metadata": {
        "id": "kshFnbLgnWza"
      },
      "outputs": [],
      "source": [
        "def verbalize_line(x):\n",
        "    if \"::\" in x:\n",
        "        p = float(x.split(\"::\")[0])\n",
        "        prefix = f\"{map[get_prob_word(p)]} that\"\n",
        "    else:\n",
        "        p = 1\n",
        "        prefix = \"\"\n",
        "    facts = re.findall(\"fact.\", x)\n",
        "    if \":-\" in x:\n",
        "        op = x.split(\":-\")[1].split(\"(\")[0]\n",
        "        if len(facts) == 3:\n",
        "            midfix = f\"{prefix} if\"\n",
        "            suffix = f\"then {facts[0]}\"\n",
        "        else:\n",
        "            suffix = midfix = \"\"\n",
        "        if op == \"and\":\n",
        "            return f\"{midfix} '{facts[-2]} and {facts[-1]}' {suffix}.\"\n",
        "        if op == \"or\":\n",
        "            return f\"{midfix} '{facts[-2]}' or '{facts[-1]}' or both {suffix}.\"\n",
        "        if op == \"xor\":\n",
        "            return f\"{midfix} either '{facts[-2]}' or '{facts[-1]}' but not both {suffix}.\"\n",
        "    else:\n",
        "        return f\"{prefix} {facts[0]}\"\n",
        "\n",
        "\n",
        "def score(s_):\n",
        "    conclusion = problog.logic.Term(\"conclusion\")\n",
        "    return get_evaluatable().create_from(PrologString(s_)).evaluate()[conclusion]\n",
        "\n",
        "\n",
        "def verbalize(s_):\n",
        "    case = lambda x: x[0].upper() + x[1:]\n",
        "    s = \"\\n\".join(\n",
        "        [case(verbalize_line(x).strip()) for x in s_.split(\"\\n\") if \"fact\" in x]\n",
        "    )\n",
        "    return s\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "M7po8JszqPpp",
      "metadata": {
        "id": "M7po8JszqPpp"
      },
      "outputs": [],
      "source": [
        "s_=generate_template(hop=2)\n",
        "verbalize(s_),score(s_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c-wE3VFibyx",
      "metadata": {
        "id": "5c-wE3VFibyx"
      },
      "outputs": [],
      "source": [
        "np.mean([get_evaluatable().create_from(PrologString(generate_template())).evaluate()\n",
        "[ problog.logic.Term('conclusion')] for _ in range(100)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "CtWgq4clRlu2",
      "metadata": {
        "id": "CtWgq4clRlu2"
      },
      "outputs": [],
      "source": [
        "class Babi:\n",
        "    def __init__(self):\n",
        "        df=pd.read_csv('https://sileod.s3.eu-west-3.amazonaws.com/babi/babi-nli.csv')\n",
        "        stopwords='the to is a of and'.split()\n",
        "        df=df[df.premise.map(lambda x:type(x)==str)]\n",
        "        df=pd.DataFrame(sum(list(df.sample(1000).premise.map(lambda x: re.split(r'[.,]\\s', x))),[]),columns=['fact'])\n",
        "        df=df[~df.fact.map(lambda x: set(x.split())&{'that','and','there','afraid'}).map(bool)]\n",
        "        df['concepts']=df.fact.map(lambda x: [a for a in x.split() if a.lower() not in stopwords])\n",
        "        df=df[df.fact.map(len)<25]\n",
        "        self.df=df\n",
        "\n",
        "    def sample_disjoint(self,n=5):\n",
        "        '''sample n unrelated facts'''\n",
        "        df=self.df.copy()\n",
        "        l=[]\n",
        "        for _ in range(n):\n",
        "            x=df.sample().iloc[0]\n",
        "            l+=[x.fact if x.fact.endswith('.') else x.fact+'.']\n",
        "            df=df[~df.concepts.map(lambda y:bool(set(y)&set(x.concepts)))]\n",
        "        return l"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "K7kXGZB_zA6H",
      "metadata": {
        "id": "K7kXGZB_zA6H"
      },
      "outputs": [],
      "source": [
        "def get_split(x):\n",
        "    if x<0.8:\n",
        "        return 'train'\n",
        "    if x<0.9:\n",
        "        return 'validation'\n",
        "    else:\n",
        "        return 'test'\n",
        "\n",
        "def punctation_fix(s):\n",
        "    for x in [',.','...',', .','..']:\n",
        "        s=s.replace(x,'.')\n",
        "    return s.rstrip('.,')+'.'\n",
        "\n",
        "def composition_reasoning(n=5000,hop=2):\n",
        "    data = Babi()\n",
        "    l = []\n",
        "    for _ in tqdm(range(n)):\n",
        "        s_ = generate_template(hop)\n",
        "        s = verbalize(s_).replace('Fact','fact')\n",
        "        probability = score(s_)\n",
        "\n",
        "        placeholders = list(set(re.findall(\"fact.\", s, flags=re.IGNORECASE)))\n",
        "        facts = data.sample_disjoint(len(placeholders))\n",
        "        for placeholder, fact in zip(placeholders, facts):\n",
        "            s = s.replace(placeholder, fact.replace(\".\", \",\"))\n",
        "        context = punctation_fix(\". \".join(s.split(\"\\n\")[:-1]))\n",
        "        hypothesis = punctation_fix(s.split(\"\\n\")[-1])\n",
        "\n",
        "\n",
        "        l += [dict_of(context, hypothesis, probability)]\n",
        "    df = pd.DataFrame(l)\n",
        "    df[\"rnd\"] = sorted(np.linspace(0,1,len(df)),key=lambda x:random.random())\n",
        "    df['split']=df.rnd.map(get_split)\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hbmueJSpaL7B",
      "metadata": {
        "id": "hbmueJSpaL7B"
      },
      "outputs": [],
      "source": [
        "def reformat(x):\n",
        "    x['label']=random.randint(0,1)\n",
        "    x['hypothesis_assertion']= x['hypothesis']\n",
        "    x['hypothesis']=[x.invalid_hypothesis,x.valid_hypothesis][x['label']]\n",
        "    return x\n",
        "\n",
        "def verbalize_hypothesis(df):\n",
        "    df['probability_word']=df.probability.map(get_prob_word)\n",
        "    df['distractor']=df.probability_word.map(lambda x:random.choice(distractors[x]))\n",
        "    lcase=lambda x:x[0].lower()+x[1:]\n",
        "    ucase=lambda x:x[0].upper()+x[1:]\n",
        "\n",
        "    df['valid_hypothesis'] = df.apply(lambda x: f'{ucase(map[x.probability_word])} that {lcase(x.hypothesis)}',axis=1)\n",
        "    df['invalid_hypothesis'] = df.apply(lambda x: f'{ucase(map[x.distractor])} that {lcase(x.hypothesis)}',axis=1)\n",
        "    df=df.apply(reformat,axis=1)\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "IFK20ZqPLaKg",
      "metadata": {
        "id": "IFK20ZqPLaKg"
      },
      "outputs": [],
      "source": [
        "df=verbalize_hypothesis(composition_reasoning(hop=1))\n",
        "for split in ['train','test','validation']:\n",
        "    df[df['split']==split].to_csv(f'{output_path}/reasoning_1hop_{split}.csv',index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9411826c",
      "metadata": {
        "id": "9411826c"
      },
      "outputs": [],
      "source": [
        "df=verbalize_hypothesis(composition_reasoning(hop=2))\n",
        "for split in ['train','test','validation']:\n",
        "    df[df['split']==split].to_csv(f'{output_path}/reasoning_2hop_{split}.csv',index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ROoczMC-c4De",
      "metadata": {
        "id": "ROoczMC-c4De"
      },
      "outputs": [],
      "source": [
        "for split in ['train','test','validation']:\n",
        "    df=pd.read_csv(f'{usnli_path}/{split}.csv',names=['context','hypothesis','_','probability'],header=0)\n",
        "    df=verbalize_hypothesis(df)\n",
        "    df.to_csv(f'{output_path}/usnli_{split}.csv',index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "64cf8dc0",
      "metadata": {
        "id": "64cf8dc0"
      },
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e329139",
      "metadata": {
        "id": "8e329139"
      },
      "source": [
        "# Noise robustness"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c890c2d",
      "metadata": {
        "id": "6c890c2d"
      },
      "outputs": [],
      "source": [
        "def score(T):\n",
        "    return fc.first(get_evaluatable().create_from(PrologString(T)).evaluate().values())\n",
        "\n",
        "errors=[]\n",
        "noises=[0,0.1,0.2,0.3,0.4,0.5]\n",
        "for noise in noises:\n",
        "\n",
        "    l=[]\n",
        "    for _ in range(500):\n",
        "        T=generate_template(hop=2)\n",
        "        T_=copy.deepcopy(T)\n",
        "        for p in re.findall('0\\.\\d+',T):\n",
        "            T_=T_.replace(p, str(np.clip(float(p)+np.random.randn()*noise,0,1)))\n",
        "        l+=[score(T_)-score(T)]\n",
        "    errors+=[np.mean(np.abs(l)>0.4)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10a97d8d",
      "metadata": {
        "id": "10a97d8d"
      },
      "outputs": [],
      "source": [
        "list(zip(noises,errors))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0a8d9049",
      "metadata": {
        "id": "0a8d9049"
      },
      "outputs": [],
      "source": [
        "plt.plot(noises,errors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d7ecc902",
      "metadata": {
        "id": "d7ecc902"
      },
      "outputs": [],
      "source": [
        "df=composition_reasoning(n=10,hop=2)\n",
        "print(\"\\n----\\n\".join(df.context))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}